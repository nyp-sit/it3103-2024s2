{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103-2024s2/blob/main/session-7/fine_tuning_vit_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97bf8340-2c4f-4b32-9a64-5b8ed2d6247f",
      "metadata": {
        "id": "97bf8340-2c4f-4b32-9a64-5b8ed2d6247f"
      },
      "source": [
        "# Fine-tuning a Vision Transformer Model With a Custom Biomedical Dataset (Additional Exercise)\n",
        "_Authored by: [Emre Albayrak](https://github.com/emre570)_\n",
        "\n",
        "This guide outlines the process for fine-tuning a Vision Transformer (ViT) model on a custom biomedical dataset. It includes steps for loading and preparing the dataset, setting up image transformations for different data splits, configuring and initializing the ViT model, and defining the training process with evaluation and visualization tools.\n",
        "\n",
        "## Dataset Info\n",
        "The custom dataset is hand-made, containing 780 images with 3 classes (benign, malignant, normal).\n",
        "\n",
        "![attachment:datasetinfo.png](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/102d6c23e6cc24db857fbc60186461ded6cdfb75/datasetinfo.png)\n",
        "\n",
        "## Model Info\n",
        "The model we fine-tune will be Google's [`\"vit-large-patch16-224\"`](https://huggingface.co/google/vit-large-patch16-224). It is trained on ImageNet-21k (14M images, 21.843 classes), and fine-tuned on ImageNet 2012 (1M images, 1.000 classes) at resolution 224x224. Google has several other ViT models with different image sizes and patches.\n",
        "\n",
        "Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc02613-7bc6-4cd8-aa97-a21ba1970027",
      "metadata": {
        "id": "3cc02613-7bc6-4cd8-aa97-a21ba1970027"
      },
      "source": [
        "## Getting Started\n",
        "First, let's install libraries first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7093dd4f-d0cb-44dc-935d-d54435187901",
      "metadata": {
        "id": "7093dd4f-d0cb-44dc-935d-d54435187901"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets transformers accelerate torch scikit-learn matplotlib wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5019a8-d130-4c08-9503-cd8415f50ae9",
      "metadata": {
        "id": "9b5019a8-d130-4c08-9503-cd8415f50ae9"
      },
      "source": [
        "(Optional) We will push our model to Hugging Face Hub so we must login."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5acb41-a225-44a5-8c8f-f212c615008f",
      "metadata": {
        "id": "3d5acb41-a225-44a5-8c8f-f212c615008f"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f2b730-ff8a-4d76-9baa-41c35b80fd38",
      "metadata": {
        "id": "80f2b730-ff8a-4d76-9baa-41c35b80fd38"
      },
      "source": [
        "## Dataset Preparation\n",
        "Datasets library automatically pulls images and classes from the dataset. For detailed info, you can visit [`this link`](https://huggingface.co/docs/datasets/image_load)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e69664-1bb0-4ee4-932c-2f108550bfba",
      "metadata": {
        "id": "f6e69664-1bb0-4ee4-932c-2f108550bfba"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"emre570/breastcancer-ultrasound-images\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf3176b2-65ff-44ea-bb1e-cd75b2d7609d",
      "metadata": {
        "id": "bf3176b2-65ff-44ea-bb1e-cd75b2d7609d"
      },
      "source": [
        "We got our dataset. But we don't have a validation set. To create the validation set, we will calculate the size of the validation set as a fraction of the training set based on the size of the test set. Then we split the training dataset into new training and validation subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d385bab-dad0-462d-9339-11205fcd2e63",
      "metadata": {
        "id": "4d385bab-dad0-462d-9339-11205fcd2e63"
      },
      "outputs": [],
      "source": [
        "# Get the numbers of each set\n",
        "test_num = len(dataset[\"test\"])\n",
        "train_num = len(dataset[\"train\"])\n",
        "\n",
        "val_size = test_num / train_num\n",
        "\n",
        "train_val_split = dataset[\"train\"].train_test_split(test_size=val_size)\n",
        "train_val_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dacfc60-6262-4e91-b9e9-e2af43210fde",
      "metadata": {
        "id": "7dacfc60-6262-4e91-b9e9-e2af43210fde"
      },
      "source": [
        "We got our seperated train set. Let's merge them with test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43bf130-49ad-4a4d-8230-436649045a16",
      "metadata": {
        "id": "b43bf130-49ad-4a4d-8230-436649045a16"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_val_split[\"train\"],\n",
        "    \"validation\": train_val_split[\"test\"],\n",
        "    \"test\": dataset[\"test\"]\n",
        "})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a0d7a89-099e-4925-97a5-51c91cdb7046",
      "metadata": {
        "id": "4a0d7a89-099e-4925-97a5-51c91cdb7046"
      },
      "source": [
        "Perfect! Our dataset is ready. Let's assign subsets to different variables. We will use them later for easy reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9462e82b-0a12-4b0c-b73e-8dba76041ee6",
      "metadata": {
        "id": "9462e82b-0a12-4b0c-b73e-8dba76041ee6"
      },
      "outputs": [],
      "source": [
        "train_ds = dataset['train']\n",
        "val_ds = dataset['validation']\n",
        "test_ds = dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27e5451f-e2fb-4d29-b63a-a8960686dcc2",
      "metadata": {
        "id": "27e5451f-e2fb-4d29-b63a-a8960686dcc2"
      },
      "source": [
        "We can see the image is a PIL.Image with a label associated with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8ffe26-f128-4f63-962e-56349d45ff23",
      "metadata": {
        "id": "1d8ffe26-f128-4f63-962e-56349d45ff23"
      },
      "outputs": [],
      "source": [
        "train_ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "384a09b0-1c47-411f-b91a-00acdd88b06b",
      "metadata": {
        "id": "384a09b0-1c47-411f-b91a-00acdd88b06b"
      },
      "source": [
        "We can also see the features of train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e09647b-44e9-4f5f-800c-c333b0523b85",
      "metadata": {
        "id": "1e09647b-44e9-4f5f-800c-c333b0523b85"
      },
      "outputs": [],
      "source": [
        "train_ds.features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3fedab5-0e80-492c-9408-f629b230351d",
      "metadata": {
        "id": "a3fedab5-0e80-492c-9408-f629b230351d"
      },
      "source": [
        "Let's show one image from each class from dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.features['label'].names"
      ],
      "metadata": {
        "id": "5C3lKuE843U1"
      },
      "id": "5C3lKuE843U1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0]['label']"
      ],
      "metadata": {
        "id": "Rpvs1hTV4_ZN"
      },
      "id": "Rpvs1hTV4_ZN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c901865-a876-4b4b-b1f2-8895b494cafb",
      "metadata": {
        "id": "5c901865-a876-4b4b-b1f2-8895b494cafb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize a set to keep track of shown labels\n",
        "shown_labels = set()\n",
        "\n",
        "# Initialize the figure for plotting\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Loop through the dataset and plot the first image of each label\n",
        "for i, sample in enumerate(train_ds):\n",
        "    label_id = sample['label']\n",
        "    label = train_ds.features['label'].names[label_id]\n",
        "    if label not in shown_labels:\n",
        "        plt.subplot(1, len(train_ds.features['label'].names), len(shown_labels) + 1)\n",
        "        plt.imshow(sample['image'])\n",
        "        plt.title(label)\n",
        "        plt.axis('off')\n",
        "        shown_labels.add(label)\n",
        "        if len(shown_labels) == len(train_ds.features['label'].names):\n",
        "            break\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0300a4b7-4f8f-4155-bad8-42df6673eddc",
      "metadata": {
        "id": "0300a4b7-4f8f-4155-bad8-42df6673eddc"
      },
      "source": [
        "## Data Processing\n",
        "The dataset is ready. But we are not ready for fine-tuning. We will follow this procedures respectively:\n",
        "\n",
        "- **Label Mapping:** We convert between label IDs and their corresponding names, useful for model training and evaluation.\n",
        "\n",
        "- **Image Processing:** Then, we utilize the ViTImageProcessor to standardize input image sizes and applies normalization specific to the pretrained model. Also, will define different transformations for training, validation, and testing to improve model generalization using torchvision.\n",
        "\n",
        "- **Transform Functions:** Implement functions to apply the transformations to the dataset, converting images to the required format and dimensions for the ViT model.\n",
        "\n",
        "- **Data Loading:** Set up a custom collate function to properly batch images and labels, and create a DataLoader for efficient loading and batching during model training.\n",
        "\n",
        "- **Batch Preparation:** Retrieve and display the shape of data in a sample batch to verify correct processing and readiness for model input."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1463c15d-5b73-47dc-8113-a910e3cd38b9",
      "metadata": {
        "id": "1463c15d-5b73-47dc-8113-a910e3cd38b9"
      },
      "source": [
        "### Label Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a6187f-0ba0-4cd9-a9ec-f06fca3a91bf",
      "metadata": {
        "id": "43a6187f-0ba0-4cd9-a9ec-f06fca3a91bf"
      },
      "outputs": [],
      "source": [
        "id2label = {id:label for id, label in enumerate(train_ds.features['label'].names)}\n",
        "label2id = {label:id for id,label in id2label.items()}\n",
        "id2label, id2label[train_ds[0]['label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be0b208-5aad-48ff-97f5-ea484cfc8ad7",
      "metadata": {
        "id": "0be0b208-5aad-48ff-97f5-ea484cfc8ad7"
      },
      "source": [
        "### Image Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d04160-93aa-425e-a06d-58c09ec6ffbd",
      "metadata": {
        "id": "a2d04160-93aa-425e-a06d-58c09ec6ffbd"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "model_name = \"google/vit-large-patch16-224\"\n",
        "processor = ViTImageProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor"
      ],
      "metadata": {
        "id": "eHuYb7rs6Hhh"
      },
      "id": "eHuYb7rs6Hhh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136d114f-a054-467e-a034-cdddf9bf574b",
      "metadata": {
        "id": "136d114f-a054-467e-a034-cdddf9bf574b"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import CenterCrop, Compose, Normalize, RandomHorizontalFlip, RandomResizedCrop, ToTensor, Resize\n",
        "\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "normalize = Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "train_transforms = Compose([\n",
        "    RandomResizedCrop(size),\n",
        "    RandomHorizontalFlip(),\n",
        "    ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "val_transforms = Compose([\n",
        "    Resize(size),\n",
        "    CenterCrop(size),\n",
        "    ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "test_transforms = Compose([\n",
        "    Resize(size),\n",
        "    CenterCrop(size),\n",
        "    ToTensor(),\n",
        "    normalize,\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0]['image'].mode"
      ],
      "metadata": {
        "id": "jNcjCyVt8coV"
      },
      "id": "jNcjCyVt8coV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9e910499-84bf-4672-bbf0-ca78915b2821",
      "metadata": {
        "id": "9e910499-84bf-4672-bbf0-ca78915b2821"
      },
      "source": [
        "### Create transform functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ddc7ad4-bd09-4c76-ac00-ca7dafbd8417",
      "metadata": {
        "id": "5ddc7ad4-bd09-4c76-ac00-ca7dafbd8417"
      },
      "outputs": [],
      "source": [
        "def apply_train_transforms(examples):\n",
        "    examples['pixel_values'] = [train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "def apply_val_transforms(examples):\n",
        "    examples['pixel_values'] = [val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "def apply_test_transforms(examples):\n",
        "    examples['pixel_values'] = [val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ac74835-f883-46a3-877c-26265b27a325",
      "metadata": {
        "id": "1ac74835-f883-46a3-877c-26265b27a325"
      },
      "source": [
        "### Apply transform functions to each set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45f2b765-5258-4b44-b2ce-6bff952bdd1b",
      "metadata": {
        "id": "45f2b765-5258-4b44-b2ce-6bff952bdd1b"
      },
      "outputs": [],
      "source": [
        "train_ds.set_transform(apply_train_transforms)\n",
        "val_ds.set_transform(apply_val_transforms)\n",
        "test_ds.set_transform(apply_test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce586ef6-5f48-4554-8dc4-48797a977674",
      "metadata": {
        "id": "ce586ef6-5f48-4554-8dc4-48797a977674"
      },
      "outputs": [],
      "source": [
        "train_ds.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "731fccb7-7311-4175-8ce1-67141a662b1e",
      "metadata": {
        "id": "731fccb7-7311-4175-8ce1-67141a662b1e"
      },
      "outputs": [],
      "source": [
        "train_ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b06fa4-6f28-4e45-8af4-779c424583fe",
      "metadata": {
        "id": "49b06fa4-6f28-4e45-8af4-779c424583fe"
      },
      "source": [
        "Looks like we converted our pixel values into tensors.\n",
        "\n",
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f47f263-2046-4847-952d-728fa3fe5cf4",
      "metadata": {
        "id": "3f47f263-2046-4847-952d-728fa3fe5cf4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "train_dl = DataLoader(train_ds, collate_fn=collate_fn, batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aed986b0-e661-4c57-ad58-4fa73d795828",
      "metadata": {
        "id": "aed986b0-e661-4c57-ad58-4fa73d795828"
      },
      "source": [
        "### Batch Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c6d3f8-48e6-423f-8193-7571f986f103",
      "metadata": {
        "id": "14c6d3f8-48e6-423f-8193-7571f986f103"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dl))\n",
        "for k,v in batch.items():\n",
        "  if isinstance(v, torch.Tensor):\n",
        "    # print(k, v.shape)\n",
        "    print(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35379d35-e567-4a30-8b91-6eac80b79044",
      "metadata": {
        "id": "35379d35-e567-4a30-8b91-6eac80b79044"
      },
      "source": [
        "Perfect! Now we are ready for fine-tuning process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45056d5e-8bca-4ece-b29b-bc772aeef49f",
      "metadata": {
        "id": "45056d5e-8bca-4ece-b29b-bc772aeef49f"
      },
      "source": [
        "## Fine-tuning the Model\n",
        "Now we will configure and fine-tune the model. We started by initializing the model with specific label mappings and pre-trained settings, adjusting for size mismatches. Training parameters are set up to define the model's learning process, including the save strategy, batch sizes, and training epochs, with results logged via Weights & Biases. Hugging Face Trainer will then instantiate to manage the training and evaluation, utilizing a custom data collator and the model's built-in processor. Finally, after training, the model's performance is evaluated on a test dataset, with metrics printed to assess its accuracy.\n",
        "\n",
        "First, we call our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d0b588-3e41-4852-9307-9e2ec7d5bb0b",
      "metadata": {
        "id": "97d0b588-3e41-4852-9307-9e2ec7d5bb0b"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(model_name, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f773e18-63e8-41bd-885f-7ba95d074a3d",
      "metadata": {
        "id": "2f773e18-63e8-41bd-885f-7ba95d074a3d"
      },
      "source": [
        "There is a subtle detail in here. The `ignore_mismatched_sizes` parameter.\n",
        "\n",
        "When you fine-tune a pre-trained model on a new dataset, sometimes the input size of your images or the model architecture specifics (like the number of labels in the classification layer) might not match exactly with what the model was originally trained on. This can happen for various reasons, such as when using a model trained on one type of image data (like natural images from ImageNet) on a completely different type of image data (like medical images or specialized camera images).\n",
        "\n",
        "Setting `ignore_mismatched_sizes` to `True` allows the model to adjust its layers to accommodate size differences without throwing an error.\n",
        "\n",
        "For example, the number of classes this model is trained on is 1000, which is `torch.Size([1000])` and it expects an input with `torch.Size([1000])` classes. Our dataset has 3, which is `torch.Size([3])` classes. If we give it directly, it will raise an error because the class numbers do not match."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c671bf-9978-46d8-82ef-0906d4e89d03",
      "metadata": {
        "id": "e2c671bf-9978-46d8-82ef-0906d4e89d03"
      },
      "source": [
        "Then, define training arguments from Google for this model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d473af8a-4070-48d7-aecb-1a5a90f0b63f",
      "metadata": {
        "id": "d473af8a-4070-48d7-aecb-1a5a90f0b63f"
      },
      "source": [
        "(Optional) Note that the metrics will be saved in Weights & Biases because we set the `report_to` parameter to `wandb`. W&B will ask you for an API key, so you should create an account and an API key. If you don't want, you can remove `report_to` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f16fc568-9fdc-4c60-acec-6ed3dbb85aef",
      "metadata": {
        "id": "f16fc568-9fdc-4c60-acec-6ed3dbb85aef"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    output_dir = \"output-models\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"wandb\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=40,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='logs',\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f74d4457-49fd-4e1b-9842-a3759ec524c9",
      "metadata": {
        "id": "f74d4457-49fd-4e1b-9842-a3759ec524c9"
      },
      "source": [
        "We can now begin the fine-tuning process with `Trainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a117d62-9054-4e14-b7b2-0703de17a741",
      "metadata": {
        "id": "5a117d62-9054-4e14-b7b2-0703de17a741"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    train_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=collate_fn,\n",
        "    tokenizer=processor,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e154fd79-1de7-4169-a6af-402b12881042",
      "metadata": {
        "id": "e154fd79-1de7-4169-a6af-402b12881042"
      },
      "source": [
        "| Epoch | Training Loss | Validation Loss | Accuracy |\n",
        "|-------|---------------|-----------------|----------|\n",
        "| 40    | 0.174700      | 0.596288        | 0.903846 |\n",
        "\n",
        "The fine-tuning process is done. Let's continue with evaluating the model to test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c19b6d99-0a89-45ac-a6d9-ec3e79edc041",
      "metadata": {
        "id": "c19b6d99-0a89-45ac-a6d9-ec3e79edc041"
      },
      "outputs": [],
      "source": [
        "outputs = trainer.predict(test_ds)\n",
        "print(outputs.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c4ddecb-7ab0-493e-90b9-44bf4e2a530e",
      "metadata": {
        "id": "2c4ddecb-7ab0-493e-90b9-44bf4e2a530e"
      },
      "source": [
        "`{'test_loss': 0.3219967782497406, 'test_accuracy': 0.9102564102564102, 'test_runtime': 4.0543, 'test_samples_per_second': 38.478, 'test_steps_per_second': 9.619}`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be50a0b",
      "metadata": {
        "id": "0be50a0b"
      },
      "source": [
        "### (Optional) Push Model to Hub\n",
        "We can push our model to Hugging Face Hub using `push_to_hub`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d55e6a",
      "metadata": {
        "id": "f1d55e6a"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"your_model_name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f74c058-e2a5-4c9c-8d70-1c2c574b933f",
      "metadata": {
        "id": "5f74c058-e2a5-4c9c-8d70-1c2c574b933f"
      },
      "source": [
        "That's great! Let's visualize the results.\n",
        "\n",
        "## Results\n",
        "We made the fine-tuning. Let's see how our model predicted the classes using scikit-learn's Confusion Matrix Display and show Recall Score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ade5321d-ff63-4308-8317-d1e4da2219df",
      "metadata": {
        "id": "ade5321d-ff63-4308-8317-d1e4da2219df"
      },
      "source": [
        "### What is Confusion Matrix?\n",
        "A confusion matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning model, on a set of test data for which the true values are known. It's especially useful for checking how well a classification model is performing because it shows the frequency of true versus predicted labels.\n",
        "\n",
        "Let's draw our model's Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8efb0ece-92b3-498d-b47b-0f9c04d4ebb8",
      "metadata": {
        "id": "8efb0ece-92b3-498d-b47b-0f9c04d4ebb8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_true = outputs.label_ids\n",
        "y_pred = outputs.predictions.argmax(1)\n",
        "\n",
        "labels = train_ds.features['label'].names\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(xticks_rotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9178d2d1-b828-45a6-8873-039abc0419c2",
      "metadata": {
        "id": "9178d2d1-b828-45a6-8873-039abc0419c2"
      },
      "source": [
        "### What is Recall Score?\n",
        "The recall score is a performance metric used in classification tasks to measure the ability of a model to correctly identify all relevant instances within a dataset. Specifically, recall assesses the proportion of actual positives that are correctly predicted as such by the model.\n",
        "\n",
        "Let's print recall scores using scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d87ca7-8458-41d5-a773-38e2c9522f64",
      "metadata": {
        "id": "48d87ca7-8458-41d5-a773-38e2c9522f64"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Calculate the recall scores\n",
        "# 'None' calculates recall for each class separately\n",
        "recall = recall_score(y_true, y_pred, average=None)\n",
        "\n",
        "# Print the recall for each class\n",
        "for label, score in zip(labels, recall):\n",
        "    print(f'Recall for {label}: {score:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b1a1b1-7de4-4eb6-98bf-de87e8cbbcec",
      "metadata": {
        "id": "c8b1a1b1-7de4-4eb6-98bf-de87e8cbbcec"
      },
      "source": [
        "`Recall for benign: 0.90,\n",
        "Recall for malignant: 0.86,\n",
        "Recall for normal: 0.78`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b76567-039d-467b-9cfc-0837fb3e1a1b",
      "metadata": {
        "id": "67b76567-039d-467b-9cfc-0837fb3e1a1b"
      },
      "source": [
        "## Conclusion\n",
        "In this cookbook, we covered how to train a ViT model with a medical dataset. It covers crucial steps such as dataset preparation, image preprocessing, model configuration, training, evaluation, and result visualization. By leveraging Hugging Face's Transformers library scikit-learn and PyTorch Torchvision, it facilitates efficient model training and evaluation, providing valuable insights into the model's performance and its ability to classify biomedical images accurately."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}