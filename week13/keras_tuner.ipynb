{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keras_tuner.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week13/keras_tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Introduction to the Keras Tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called *hyperparameter tuning* or *hypertuning*.\n",
        "\n",
        "Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\n",
        "1. **Model hyperparameters** which influence model selection such as the number and width of hidden layers\n",
        "2. **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier\n",
        "\n",
        "In this lab, you will use the Keras Tuner to perform hypertuning for an image classification application.\n",
        "\n",
        "\n",
        "*Acknowledgement*: This notebook is adapted from https://www.tensorflow.org/tutorials/keras/keras_tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:48:05.522351Z",
          "iopub.status.busy": "2021-06-16T14:48:05.521758Z",
          "iopub.status.idle": "2021-06-16T14:48:07.011500Z",
          "shell.execute_reply": "2021-06-16T14:48:07.010890Z"
        },
        "id": "IqR2PQG4ZaZ0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g83Lwsy-Aq2_"
      },
      "source": [
        "Install and import the Keras Tuner and also the logger library that allows the hyper-parameters search to be visualized in Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:48:07.021492Z",
          "iopub.status.busy": "2021-06-16T14:48:07.015205Z",
          "iopub.status.idle": "2021-06-16T14:48:10.233481Z",
          "shell.execute_reply": "2021-06-16T14:48:10.232963Z"
        },
        "id": "hpMLpbt9jcO6"
      },
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41uChua5R5_t"
      },
      "source": [
        "# Clean up any previous logs\n",
        "!rm -rf ./logs\n",
        "!rm -rf ./my_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:48:10.237808Z",
          "iopub.status.busy": "2021-06-16T14:48:10.237199Z",
          "iopub.status.idle": "2021-06-16T14:48:10.725534Z",
          "shell.execute_reply": "2021-06-16T14:48:10.725887Z"
        },
        "id": "_leAIdFKAxAD"
      },
      "source": [
        "import keras_tuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReV_UXOgCZvx"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "In this tutorial, you will use the Keras Tuner to find the best hyperparameters for a machine learning model that classifies images of clothing from the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HljH_ENLEdHa"
      },
      "source": [
        "Load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:48:10.730332Z",
          "iopub.status.busy": "2021-06-16T14:48:10.729737Z",
          "iopub.status.idle": "2021-06-16T14:48:11.134927Z",
          "shell.execute_reply": "2021-06-16T14:48:11.135316Z"
        },
        "id": "OHlHs9Wj_PUM"
      },
      "source": [
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:48:11.139286Z",
          "iopub.status.busy": "2021-06-16T14:48:11.138688Z",
          "iopub.status.idle": "2021-06-16T14:48:11.219754Z",
          "shell.execute_reply": "2021-06-16T14:48:11.219220Z"
        },
        "id": "bLVhXs3xrUD0"
      },
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5YEL2H2Ax3e"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a *hypermodel*.\n",
        "\n",
        "You can define a hypermodel through two approaches:\n",
        "\n",
        "* By using a model builder function\n",
        "* By subclassing the `HyperModel` class of the Keras Tuner API\n",
        "\n",
        "You can also use two pre-defined `HyperModel` classes - [HyperXception](https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperxception-class) and [HyperResNet](https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperresnet-class) for computer vision applications.\n",
        "\n",
        "In this tutorial, you use a model builder function to define the image classification model. The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:48:11.225900Z",
          "iopub.status.busy": "2021-06-16T14:48:11.225299Z",
          "iopub.status.idle": "2021-06-16T14:48:11.227310Z",
          "shell.execute_reply": "2021-06-16T14:48:11.226912Z"
        },
        "id": "ZQKodC-jtsva"
      },
      "source": [
        "def model_builder(hp):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
        "  for i in range(hp.Int('num_layers', 1, 2)):\n",
        "    model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i), 32, 64, step=16),\n",
        "                                    activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J1VYw4q3x0b"
      },
      "source": [
        "## Instantiate the tuner and perform hypertuning\n",
        "\n",
        "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. In this tutorial, you use the [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) tuner.\n",
        "\n",
        "To instantiate the Hyperband tuner, you must specify the hypermodel, the `objective` to optimize and the maximum number of epochs to train (`max_epochs`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:48:11.231196Z",
          "iopub.status.busy": "2021-06-16T14:48:11.230635Z",
          "iopub.status.idle": "2021-06-16T14:48:12.886657Z",
          "shell.execute_reply": "2021-06-16T14:48:12.886149Z"
        },
        "id": "oichQFly6Y46"
      },
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=30,\n",
        "                     factor=3,\n",
        "                     project_name='intro_to_kt',\n",
        "                     directory='my_dir')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIhhdKf9VtI"
      },
      "source": [
        "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + log<sub>`factor`</sub>(`max_epochs`) and rounding it up to the nearest integer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKghEo15Tduy"
      },
      "source": [
        "Run the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:48:12.897064Z",
          "iopub.status.busy": "2021-06-16T14:48:12.896523Z",
          "iopub.status.idle": "2021-06-16T14:53:44.476814Z",
          "shell.execute_reply": "2021-06-16T14:53:44.476321Z"
        },
        "id": "dSBQcTHF9cKt"
      },
      "source": [
        "tuner.search(img_train, label_train, epochs=30, validation_split=0.2, callbacks=[keras.callbacks.TensorBoard(\"logs/hparams\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz8gQEwwyP7I"
      },
      "source": [
        "# Get the optimal hyperparameters\n",
        "\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_num_layers = best_hps.get('num_layers')\n",
        "print(f'best number of layers = {best_num_layers}')\n",
        "\n",
        "for i in range(best_num_layers):\n",
        "    best_units = best_hps.get('units_' + str(i))\n",
        "    print(f'best number of units for layer {i} = {best_units}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSyjWQ3mPKT9"
      },
      "source": [
        "## Visualize the results in TensorBoard's HParams plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VBvplwyP8Vy"
      },
      "source": [
        "The HParams dashboard can now be opened. Start TensorBoard and click on \"HParams\" at the top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-OaOJcq243R"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/hparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RPGbR9EWd4o"
      },
      "source": [
        "The left pane of the dashboard provides filtering capabilities that are active across all the views in the HParams dashboard:\n",
        "\n",
        "- Filter which hyperparameters/metrics are shown in the dashboard\n",
        "- Filter which hyperparameter/metrics values are shown in the dashboard\n",
        "- Filter on run status (running, success, ...)\n",
        "- Sort by hyperparameter/metric in the table view\n",
        "- Number of session groups to show (useful for performance when there are many experiments)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Q5v28XaCt8"
      },
      "source": [
        "The HParams dashboard has three different views, with various useful information:\n",
        "\n",
        "* The **Table View** lists the runs, their hyperparameters, and their metrics.\n",
        "* The **Parallel Coordinates View** shows each run as a line going through an axis for each hyperparemeter and metric. Click and drag the mouse on any axis to mark a region which will highlight only the runs that pass through it. This can be useful for identifying which groups of hyperparameters are most important. The axes themselves can be re-ordered by dragging them.\n",
        "* The **Scatter Plot View** shows plots comparing each hyperparameter/metric with each metric. This can help identify correlations. Click and drag to select a region in a specific plot and highlight those sessions across the other plots. \n",
        "\n",
        "A table row, a parallel coordinates line, and a scatter plot market can be clicked to see a plot of the metrics as a function of training steps for that session (although in this tutorial only one step is used for each run)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lak_ylf88xBv"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Now we will train the model with the hyperparameters obtained from the search, and to save the best checkpoint (based on validation accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:53:44.573695Z",
          "iopub.status.busy": "2021-06-16T14:53:44.481848Z",
          "iopub.status.idle": "2021-06-16T14:55:48.784982Z",
          "shell.execute_reply": "2021-06-16T14:55:48.784541Z"
        },
        "id": "McO82AXOuxXh"
      },
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 30 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZBNur7SKnzk"
      },
      "source": [
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"bestcheckpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "history = model.fit(img_train, label_train, epochs=30, validation_split=0.2, callbacks=[model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqU5ZVAaag2v"
      },
      "source": [
        "To finish this lab, evaluate the hypermodel on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T14:57:32.790341Z",
          "iopub.status.busy": "2021-06-16T14:57:32.789201Z",
          "iopub.status.idle": "2021-06-16T14:57:33.472839Z",
          "shell.execute_reply": "2021-06-16T14:57:33.473275Z"
        },
        "id": "9E0BTp9Ealjb"
      },
      "source": [
        "model.load_weights('bestcheckpoint')\n",
        "eval_result = model.evaluate(img_test, label_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQRpPHZsz-eC"
      },
      "source": [
        "The `my_dir/intro_to_kt` directory contains detailed logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional `overwrite=True` argument while instantiating the tuner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKwLOzKpFGAj"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Using the best hyper-parameters found earlier (number of layers, and number of units), modify the codes above to perform hyper-parameter search on the learning rate of Adam optimizer, with the following search space `\\[0.001, 0.01\\]`. \n",
        "\n",
        "You can use [`hp.Choice()`](https://keras.io/api/keras_tuner/hyperparameters/#choice-method) to search among the list of choices. \n",
        "\n",
        "Change the tuner to use [BayesianOptimization](https://keras.io/api/keras_tuner/tuners/bayesian/). Use the default parameter values for the BayesianOptimization search.\n",
        "\n",
        "Which learning rate give you the best validation accuracy?"
      ]
    }
  ]
}