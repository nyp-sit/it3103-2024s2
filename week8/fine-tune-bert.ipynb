{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024s2/blob/main/session-5/fine-tune-bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning BERT for Text Classification\n",
        "\n",
        "One of the approaches where we can use BERT for downstream task such as text classification is to do fine-tuning of the pretrained model.\n",
        "\n",
        "In this lab, we will see how we can use a pretrained DistilBert Model and fine-tune it with custom training data for text classification task.\n",
        "\n",
        "At the end of this session, you will be able to:\n",
        "\n",
        "prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
        "configure the transformer model for fine-tuning\n",
        "train the model for binary and multi-class text classification"
      ],
      "metadata": {
        "id": "GEnB8NCyW21V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Transformers and other libraries\n",
        "\n",
        "If you are running this notebook in Google Colab, you will need to install the Hugging Face transformers library as it is not part of the standard environment.\n"
      ],
      "metadata": {
        "id": "j62zV1XXXUPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48spsQogssNh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets>=2.18.0 transformers>=4.38.2 sentence-transformers>=2.5.1 setfit>=1.0.3 accelerate>=0.27.2 seqeval>=1.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBeVnXxQWy7-"
      },
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "The train set has 40000 samples. We will use only a small subset (e.g. 2500) samples for finetuning our pretrained model. Similarly we will use a smaller test set for evaluating our model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# downloaded the datasets.\n",
        "test_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv'\n",
        "train_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv'\n",
        "\n",
        "train_data = load_dataset('csv', data_files=train_data_url, split=\"train\").shuffle(seed=128).select(range(2500))\n",
        "test_data = load_dataset('csv', data_files=test_data_url, split=\"train\").shuffle(seed=128).select(range(500))"
      ],
      "metadata": {
        "id": "ZWIl9VBV8_wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-1UGAg7WAHk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load Model and Tokenizer\n",
        "model_id = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Let us take a closer look at the output of the tokenization process.\n",
        "\n",
        "We notice that the tokenizer will return a dictionary of two items 'input_ids' and 'attention_mask'. The input_ids contains the IDs of the tokens. While the 'attention_mask' contains the masking pattern for those padded positions. If you are using BERT tokenizer, there will be additional item called 'token_type_ids'.\n",
        "\n",
        "We also notice that for the example sentence, the word 'Transformer' is being broken up into two tokens 'Trans' and '##former'. The '##' means that the rest of the token should be attached to the previous one.\n",
        "\n",
        "We also see that the tokenizer appended [CLS] (token_id=101) to the beginning of the token sequence, and [SEP] (token_id=102) at the end."
      ],
      "metadata": {
        "id": "TSR6FC9qbdSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"Transformer is really good for Natural Language Processing.\"\n",
        "\n",
        "encoding = tokenizer(test_sentence, padding=True, truncation=True)\n",
        "print(f\"Encoding keys:  {encoding.keys()}\\n\")\n",
        "\n",
        "print(f\"token ids: {encoding['input_ids']}\\n\")\n",
        "print(f\"attention_mask: {encoding['attention_mask']}\\n\")\n",
        "print(f\"tokens: {tokenizer.decode(encoding['input_ids'])}\")"
      ],
      "metadata": {
        "id": "LNRUxqVQbmRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r48vDo8fa33D"
      },
      "source": [
        "###Create the tokenized dataset\n",
        "\n",
        "We will first convert the sentiment label from text to numeric label. We will need to create a data field called 'label' as the model will use this 'label' key as the target label.\n",
        "\n",
        "We will also use the model's (in this case the DistilBERT) tokenizer to produce the input data that are suitable to be used by the DistilBert model, e.g. the input_ids, the attention_mask.  It automatically append the [CLS] token in the front of the sequence of token_ids and the [SEP] token at the end of the sequence of token_ids , and also the attention mask for those padded positions in the input sequence of tokens.\n",
        "\n",
        "We also specify the DataCollator to use. Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset. To be able to build batches, data collators may apply some processing (like padding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ySNm-a3WCFI"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# Pad to the longest sequence in the batch\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def convert_label(sample):\n",
        "    return {\n",
        "        \"label\" : 0 if sample['sentiment'] == 'negative' else 1\n",
        "    }\n",
        "    # sample['sentiment'] = 0 if sample['sentiment'] == 'negative' else 1\n",
        "    # return { \"label\": sample['sentiment']}\n",
        "    # return sample\n",
        "    # return {\n",
        "    #     \"text\":  sample['review'],\n",
        "    #     \"label\": sample['sentiment'] }\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Tokenize input data\"\"\"\n",
        "    return tokenizer(examples[\"review\"], truncation=True)\n",
        "\n",
        "\n",
        "# Tokenize train/test data\n",
        "tokenized_train = train_data.map(convert_label).map(preprocess_function, remove_columns=['review', 'sentiment'], batched=True)\n",
        "tokenized_test = test_data.map(convert_label).map(preprocess_function, remove_columns=['review', 'sentiment'], batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_test)"
      ],
      "metadata": {
        "id": "P1CDrutbMx_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ1NM1gjbMD7"
      },
      "source": [
        "We will define a compute_metrics() function to calculate the necessary metrics. With compute_metrics we can define any number of metrics that we are\n",
        "interested in and that can be printed out or logged during training. This is\n",
        "especially helpful during training as it allows for detecting overfitting\n",
        "behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y724gUYyWIvq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Calculate F1 score\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    load_f1 = evaluate.load(\"f1\")\n",
        "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "    return {\"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the Model\n",
        "\n",
        "We will instantiate a pretrained model 'distilbert-base-uncased', using AutoModelForSequenceClassification.\n",
        "\n",
        "We define the number of labels that we want to predict beforehand. This is\n",
        "necessary to create the feedforward neural network that is applied on top of\n",
        "our pretrained model:"
      ],
      "metadata": {
        "id": "8mkGGPpqiu25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)"
      ],
      "metadata": {
        "id": "l6PB6FxBjXAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer models benefit from a much lower learning rate than the default used by AdamW, which is 0.001. In this training, we will start the training with 2e-5 (0.00002) and slowly reduce the learning rate over the course of training. In the literature, you will sometimes see this referred to as decaying or annealing the learning rate."
      ],
      "metadata": {
        "id": "BcTEjvDFjwo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# go to https://wandb.ai/authorize to get your access key\n",
        "os.environ['WANDB_API_KEY']=\"<<your wandb access key>\"\n",
        "os.environ['WANDB_PROJECT']=\"transformer_proj\""
      ],
      "metadata": {
        "id": "_sMEYfY0rc1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dho6VcG9WK5u"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Training arguments for parameter tuning\n",
        "training_args = TrainingArguments(\n",
        "   \"model\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=1,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        "   eval_strategy='steps',\n",
        "   eval_steps=0.1,\n",
        "   report_to=\"wandb\",\n",
        "   logging_steps=0.1,\n",
        "   run_name=\"bert-finetune\"\n",
        ")\n",
        "\n",
        "# Trainer which executes the training process\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOzl0WnSbVnY"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkBUVlUYbUnn"
      },
      "source": [
        "Evaluate results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCI9uYDObWU8"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5gefwxOBllA"
      },
      "source": [
        "### Freeze Layers\n",
        "\n",
        "To show the importance of training the entire network, we will now freeze the main DistilBERT model and allow only updates to pass through the classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0ZOuoe7Dj3c"
      },
      "outputs": [],
      "source": [
        "# Load Model and Tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our pretrained DistilBERT model contains a lot of layers that we can potentially\n",
        "freeze. Inspecting these layers gives insight into the structure of the network\n",
        "and what we might want to freeze:"
      ],
      "metadata": {
        "id": "N9O7V2sXI5cP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI8vf_mnBniu"
      },
      "outputs": [],
      "source": [
        "# Print layer names\n",
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnpGOry_Bm36"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "\n",
        "     # Trainable classification head\n",
        "     if name.startswith(\"classifier\") or name.startswith(\"pre_classifier\"):\n",
        "        param.requires_grad = True\n",
        "\n",
        "      # Freeze everything else\n",
        "     else:\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf_wYzpMB4uX"
      },
      "outputs": [],
      "source": [
        "# We can check whether the model was correctly updated\n",
        "for name, param in model.named_parameters():\n",
        "     print(f\"Parameter: {name} ----- {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVi36FJSG4ue"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Trainer which executes the training process\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCPpixB1HCsI"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw729mLhIQL6"
      },
      "source": [
        "### Freeze blocks 1-4\n",
        "\n",
        "Instead of freezing nearly all layers, let’s freeze everything up until encoder block 4 and see how it affects performance. A major benefit is that this reduces computation but still allows updates to flow through part of the\n",
        "pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbsLR561Kje-"
      },
      "outputs": [],
      "source": [
        "# We can check whether the model was correctly updated\n",
        "for index, (name, param) in enumerate(model.named_parameters()):\n",
        "     print(f\"Parameter: {index}{name} ----- {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyleqOHICBjj"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Encoder block 10 starts at index 68 and\n",
        "# we freeze everything before that block\n",
        "for index, (name, param) in enumerate(model.named_parameters()):\n",
        "    if index < 68:\n",
        "        param.requires_grad = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAfBhqVwC61e"
      },
      "outputs": [],
      "source": [
        "# Trainer which executes the training process\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}